{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42137056",
   "metadata": {},
   "source": [
    "## Step1. 데이터 수집하기\n",
    "- https://github.com/sunnysai12345/News_Summary\n",
    "\n",
    "```bash\n",
    "aiffel\n",
    "└── news_summarization\n",
    "    ├── project12.ipynb            <─ I am here\n",
    "    └── data\n",
    "        └── news_summary_more.csv  <─ 뉴스 기사 데이터\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3fdde",
   "metadata": {},
   "source": [
    "## csv파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc7bd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수: 98401\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_csv_data = pd.read_csv('data/news_summary_more.csv', encoding='iso-8859-1')\n",
    "print('전체 샘플 수:', (len(news_csv_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628c7f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_csv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc30d12",
   "metadata": {},
   "source": [
    "`news_summary_more.csv`는 headlines 컬럼과 text 컬럼으로 이루어져 있습니다.  \n",
    " 추상적 요약을 하는 경우 본문(text 컬럼)과 이미 요약된 데이터(headlines 컬럼)를 모델 학습에 사용합니다.  \n",
    " 추출적 요약을 하는 경우 text 컬럼만 사용하면 되지만, 추상적 요약을 해야하기에 두 컬럼 모두를 사용합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43a3f0",
   "metadata": {},
   "source": [
    "## 반복문 진행상황 확인하기\n",
    "언제 끝날지 알 수 없는 전처리 반복 과정을 `tqdm`라이브러리를 활용하여 진행상황을 보면 덜 답답하지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926d358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tqdm import notebook\n",
    "except:\n",
    "    print(\"tqdm 모듈이 없어 설치합니다.\")\n",
    "    !pip install tqdm\n",
    "    from tqdm import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc7c3d",
   "metadata": {},
   "source": [
    "## Numpy 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3098a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e43559",
   "metadata": {},
   "source": [
    "## Step2. 데이터 전처리하기 (추상적 요약)\n",
    "실습에서 사용된 전처리 또는 필요하다고 생각한 전처리를 추가하여 텍스트를 정규화 또는 정제하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b4f74",
   "metadata": {},
   "source": [
    "### 중복 샘플과 Null값이 존재하는지 확인하기\n",
    "\n",
    "#### 중복 샘플 확인하고 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf81870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n",
      "text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n"
     ]
    }
   ],
   "source": [
    "print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', news_csv_data['headlines'].nunique())\n",
    "print('text 열에서 중복을 배제한 유일한 샘플의 수 :', news_csv_data['text'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550cb21b",
   "metadata": {},
   "source": [
    "headlines에서는 98,280개, text에는 98,360개의 유니크한 데이터가 존재합니다.  \n",
    "text 자체가 중복된 경우는 중복 샘플이므로 제거를 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054979b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "news_csv_data.drop_duplicates(subset = ['text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(news_csv_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa0dcc",
   "metadata": {},
   "source": [
    "#### Null 값을 가진 샘플 확인하고 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf9c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(news_csv_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdbfe99",
   "metadata": {},
   "source": [
    "다행히 Null값이 존재하지 않네요!  \n",
    "바로 다음 단계로 넘어가겠습니다.  \n",
    "만약 Null값이 있었다면 아래와 같은 전처리 작업을 진행하면 됩니다.  \n",
    "```python\n",
    "news_csv_data.dropna(axis=0, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0597840",
   "metadata": {},
   "source": [
    "### 텍스트 정규화와 불용어 제거하기\n",
    "\n",
    "#### 정규화 사전 준비\n",
    "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f3e5f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c67cb",
   "metadata": {},
   "source": [
    "#### 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaaaa007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce74f27",
   "metadata": {},
   "source": [
    "#### 데이터 전처리  함수 선언하기\n",
    "불용어 제거 외에도 텍스트 소문자화, 특숨누자 제거 등 필요한 텍스트 전처리 작업을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04309763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27e428",
   "metadata": {},
   "source": [
    "##### text 컬럼 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd3652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1cf14b313c4d7d941f6706ee958c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_text = []\n",
    "\n",
    "for text in notebook.tqdm(news_csv_data['text']):\n",
    "    _cleaned = preprocess_sentence(text, remove_stopwords=True)\n",
    "    clean_text.append(_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "clean_text_path = 'data/clean_text.csv'\n",
    "if os.path.isfile(claen_text_path):\n",
    "    with open(clean_text_path, 'r') as file:\n",
    "        clean_text = list(csv.reader(file))\n",
    "    else:\n",
    "        with open(clean_text_path, 'w') as file:\n",
    "            write = csv.writer(file)\n",
    "            write.writerow(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5331ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text컬럼 전처리 전/후 결과 보기\n",
    "print(\"Text 전처리 전 결과\\n\", news_csv_data['text'][0])\n",
    "print(\"Text 전처리 후 결과\\n\", clean_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac3c02",
   "metadata": {},
   "source": [
    "##### headlines 컬럼 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_headlines = []\n",
    "for text in notebook.tqdm(news_csv_data['headlines']):\n",
    "    _cleaned = preprocess_sentence(text, remove_stopwords=False)\n",
    "    clean_headlines.append(_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4b57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# headlines 컬럼 전처리 전/후 결과 보기\n",
    "print(\"Text 전처리 전 결과\", news_csv_data['headlines'][0])\n",
    "print(\"Text 전처리 후 결과\", clean_headlines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec270b9",
   "metadata": {},
   "source": [
    "전처리 작업 후 빈(empty) 데이터가 생겼다면 이를 Null로 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data['text'] = clean_text\n",
    "news_csv_data['headlines'] = clean_headlines\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "news_csv_data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de49f5",
   "metadata": {},
   "source": [
    "##### Null을 가진 데이터가 있는지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_csv_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf563ac",
   "metadata": {},
   "source": [
    "이번에도 다행히 Null 데이터는 발견되지 않았습니다! 짝짝!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2e300",
   "metadata": {},
   "source": [
    "### 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in news_csv_data['text']]\n",
    "headlines_len = [len(s.split()) for s in news_csv_data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('헤드라인의 최소 길이 : {}'.format(np.min(headlines_len)))\n",
    "print('헤드라인의 최대 길이 : {}'.format(np.max(headlines_len)))\n",
    "print('헤드라인의 평균 길이 : {}'.format(np.mean(headlines_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(headlines_len)\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('headlines')\n",
    "plt.hist(headlines_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791b94f",
   "metadata": {},
   "source": [
    "text의 경우 최소 1에서 최대 60까지 분포되어 있으며 분포도가 고른 편입니다.  \n",
    "headlines의 경우에도 최소 1에서 최대 16으로 고른 분포도를 갖고 있습니다.\n",
    "\n",
    "적절한 최대길이는 text의 경우 45 headlines의 경우 12로 정의 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434345ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 45\n",
    "headlines_max_len = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2352fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s.split()) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e89631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "below_threshold_len(text_max_len, news_csv_data['text'])\n",
    "below_threshold_len(headlines_max_len,  news_csv_data['headlines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df49af",
   "metadata": {},
   "source": [
    "### 임의로 정한 최대 길이보다 큰 샘플은 제외하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb68479",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data = news_csv_data[news_csv_data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "news_csv_data = news_csv_data[news_csv_data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727121c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('전체 샘플수 :', (len(news_csv_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1abfec",
   "metadata": {},
   "source": [
    "98,360개의 샘플이 96,871개로 소폭 감소하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164d5f5",
   "metadata": {},
   "source": [
    "### 시작 토큰과 종료 토큰 추가하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤드라인 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "news_csv_data['decoder_input'] = news_csv_data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "news_csv_data['decoder_target'] = news_csv_data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "news_csv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641582bf",
   "metadata": {},
   "source": [
    "인코더의 입력, 디코더의 입력과 레이블을 각각 다시 Numpy 타입으로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(news_csv_data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(news_csv_data['decoder_input']) # 디코더의 입력, 시작 토크\n",
    "decoder_target = np.array(news_csv_data['decoder_target']) # 디코더의 레이블, 종료 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f7932",
   "metadata": {},
   "source": [
    "### 훈련 데이터와 테스트 데이터 분리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3378d69",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터를 랜덤으로 섞기 위한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c391f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1080000",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ecc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd20e5e",
   "metadata": {},
   "source": [
    "### 단어 집합 사전 만들기 및 정수 인코딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d23394",
   "metadata": {},
   "source": [
    "#### text 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82553472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd690c03",
   "metadata": {},
   "source": [
    "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기가 22,011 이므로 여기서는 20,000 으로 __제한__ 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063bae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 20000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 20,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25725295",
   "metadata": {},
   "source": [
    "#### headlines 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88805603",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab = 10000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) # 단어 집합의 크기를 10,000으로 제한\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9330235",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a440807",
   "metadata": {},
   "source": [
    "### 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83565985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=headlines_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=headlines_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=headlines_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=headlines_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6eb76a",
   "metadata": {},
   "source": [
    "## Step3. 어텐션 메커니즘 사용하기 (추상적요약)\n",
    "어텐션 메커니즘을 사용한 seq2seq를 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47c8ee",
   "metadata": {},
   "source": [
    "### 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50203779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62241f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eef4e5",
   "metadata": {},
   "source": [
    "### 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7a01d",
   "metadata": {},
   "source": [
    "#### 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cb428",
   "metadata": {},
   "source": [
    "#### 인퍼런스 모델 구현\n",
    "테스트 단계에서 정수 인덱스 행렬로 존재하는 텍스트 데이터를 실제 데이터로 복원하는 과정  \n",
    "text 단어 집합에서 정수 -> 단어  \n",
    "headlines 단어 집합에서 단어 -> 정수  \n",
    "headlines 단어 집합에서 정수 -> 단어  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\"\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (headlines_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc581d66",
   "metadata": {},
   "source": [
    "## Step4. 실제 결과와 요약문 비교하기 (추상적요약)\n",
    "headless 컬럼의 내용과 학습을 통해 얻은 추상적 요약 결과를 비교해자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2headlines(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i !=0 and i!=tar_word_to_index['sostoken']) and i !=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038ed59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10, 20):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2headlines(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65798e",
   "metadata": {},
   "source": [
    "## Step5. Summa을 이용해서 추출적 요약해보기\n",
    "Suma의 Summarize을 사용하여 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d043edf",
   "metadata": {},
   "source": [
    "전처리까지 모두 완료한 `['text']` 컬럼의 내용이 __충분치 않은지__ Suma의 Summarize 라이브러리가 __작동하지 않았다.__  \n",
    "그래서 원본 csv파일을 토대로 추출적 요약을 진행해보았다.  \n",
    "아래 링크는 Github에 올라온 내용이다.  \n",
    "[why i got empty string #28 - Github](https://github.com/summanlp/textrank/issues/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f55d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data2 = pd.read_csv('data/news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize의 ratio를 조절하면 글자가 보이기도 안보이기도 한다. 이 비율을 잘 조정해야겠다.\n",
    "# ratio의 범위는 0 ~ 1 사이의 값을 갖는다.\n",
    "for i in range(10):\n",
    "    print(\"실제 요약 : \", news_csv_data2['headlines'][i])\n",
    "    print(\"예측 요약 : \", summarize(news_csv_data2['text'][i], ratio=0.5))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1840e13",
   "metadata": {},
   "source": [
    "### 추출적요약과 추상적 요약의 요약 성능 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b9e50",
   "metadata": {},
   "source": [
    "news_csv_data2 데이터 프레임에 추출적 요약 내용 컬럼을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data2['exp'] =news_csv_data2.apply(lambda x: summarize(x['text'], ratio=0.5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bec027",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5796d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data['abs'] = decode_sequence(encoder_input_test.reshape(1, text_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2dbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb372e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
